# -*- coding: utf-8 -*-
"""GearBox fault prediction: SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15E8pzssrmIooKK93wtv_ebE_aWMRdyoV
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from matplotlib import pyplot
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

import os
import pandas as pd

# Google Drive path to the directory containing CSV files
drive_path = '/content/drive/My Drive/GearBoxDatasets/Healthy/'

# Get a list of all CSV files in the directory
csv_files = [file for file in os.listdir(drive_path) if file.endswith('.csv')]

# Initialize an empty DataFrame to store the concatenated data
all_data = pd.DataFrame()

# Loop through each CSV file and concatenate its data to the DataFrame
for csv_file in csv_files:
    file_path = os.path.join(drive_path, csv_file)
    df = pd.read_csv(file_path)
    all_data = pd.concat([all_data, df], ignore_index=True)

# Display the concatenated DataFrame
all_data.head()

import os
import pandas as pd

# Google Drive path to the directory containing CSV files
drive_path = '/content/drive/My Drive/GearBoxDatasets/BrokenTooth/'

# Get a list of all CSV files in the directory
csv_files = [file for file in os.listdir(drive_path) if file.endswith('.csv')]

# Initialize an empty DataFrame to store the concatenated data
all_data = pd.DataFrame()

# Loop through each CSV file and concatenate its data to the DataFrame
for csv_file in csv_files:
    file_path = os.path.join(drive_path, csv_file)
    df = pd.read_csv(file_path)
    all_data = pd.concat([all_data, df], ignore_index=True)

# Display the concatenated DataFrame
all_data.head()

# prompt: define all the csv file_path with a personal variable for each file

h30hz0  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz0.csv")
h30hz10  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz10.csv")
h30hz20  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz20.csv")
h30hz30  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz30.csv")
h30hz40  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz40.csv")
h30hz50  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz50.csv")
h30hz60  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz60.csv")
h30hz70  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz70.csv")
h30hz80  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz80.csv")
h30hz90  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/Healthy/h30hz90.csv")

b30hz0  = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz0.csv")
b30hz10 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz10.csv")
b30hz20 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz20.csv")
b30hz30 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz30.csv")
b30hz40 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz40.csv")
b30hz50 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz50.csv")
b30hz60 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz60.csv")
b30hz70 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz70.csv")
b30hz80 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz80.csv")
b30hz90 = pd.read_csv("/content/drive/My Drive/GearBoxDatasets/BrokenTooth/b30hz90.csv")

h30hz0.head()

failure = 0

load = 0

h30hz0['load'] = load*np.ones((len(h30hz0.index),1))
failureArray = np.zeros((len(h30hz0.index),1))
h30hz0['failure'] = failureArray

load = 10

h30hz10['load'] = load*np.ones((len(h30hz10.index),1))
failureArray = np.zeros((len(h30hz10.index),1))
h30hz10['failure'] = failureArray

load = 20

h30hz20['load'] = load*np.ones((len(h30hz20.index),1))
failureArray = np.zeros((len(h30hz20.index),1))
h30hz20['failure'] = failureArray

load = 30

h30hz30['load'] = load*np.ones((len(h30hz30.index),1))
failureArray = np.zeros((len(h30hz30.index),1))
h30hz30['failure'] = failureArray

load = 40

h30hz40['load'] = load*np.ones((len(h30hz40.index),1))
failureArray = np.zeros((len(h30hz40.index),1))
h30hz40['failure'] = failureArray

load = 50

h30hz50['load'] = load*np.ones((len(h30hz50.index),1))
failureArray = np.zeros((len(h30hz50.index),1))
h30hz50['failure'] = failureArray

load = 60

h30hz60['load'] = load*np.ones((len(h30hz60.index),1))
failureArray = np.zeros((len(h30hz60.index),1))
h30hz60['failure'] = failureArray

load = 70

h30hz70['load'] = load*np.ones((len(h30hz70.index),1))
failureArray = np.zeros((len(h30hz70.index),1))
h30hz70['failure'] = failureArray

load = 80

h30hz80['load'] = load*np.ones((len(h30hz80.index),1))
failureArray = np.zeros((len(h30hz80.index),1))
h30hz80['failure'] = failureArray

load = 90

h30hz90['load'] = load*np.ones((len(h30hz90.index),1))
failureArray = np.zeros((len(h30hz90.index),1))
h30hz90['failure'] = failureArray

h30hz90.head()

failure = 1

load = 0

b30hz0['load'] = load*np.ones((len(b30hz0.index),1))
failureArray = np.ones((len(b30hz0.index),1))
b30hz0['failure'] = failureArray

load = 10

b30hz10['load'] = load*np.ones((len(b30hz10.index),1))
failureArray = np.ones((len(b30hz10.index),1))
b30hz10['failure'] = failureArray

load = 20

b30hz20['load'] = load*np.ones((len(b30hz20.index),1))
failureArray = np.ones((len(b30hz20.index),1))
b30hz20['failure'] = failureArray

load = 30

b30hz30['load'] = load*np.ones((len(b30hz30.index),1))
failureArray = np.ones((len(b30hz30.index),1))
b30hz30['failure'] = failureArray

load = 40

b30hz40['load'] = load*np.ones((len(b30hz40.index),1))
failureArray = np.ones((len(b30hz40.index),1))
b30hz40['failure'] = failureArray

load = 50

b30hz50['load'] = load*np.ones((len(b30hz50.index),1))
failureArray = np.ones((len(b30hz50.index),1))
b30hz50['failure'] = failureArray

load = 60

b30hz60['load'] = load*np.ones((len(b30hz60.index),1))
failureArray = np.ones((len(b30hz60.index),1))
b30hz60['failure'] = failureArray

load = 70

b30hz70['load'] = load*np.ones((len(b30hz70.index),1))
failureArray = np.ones((len(b30hz70.index),1))
b30hz70['failure'] = failureArray

load = 80

b30hz80['load'] = load*np.ones((len(b30hz80.index),1))
failureArray = np.ones((len(b30hz80.index),1))
b30hz80['failure'] = failureArray

load = 90

b30hz90['load'] = load*np.ones((len(b30hz90.index),1))
failureArray = np.ones((len(b30hz90.index),1))
b30hz90['failure'] = failureArray

# check
b30hz90.tail()

# Broken

broken_df = pd.concat([b30hz0,b30hz10,b30hz20,b30hz30,b30hz40,b30hz50,b30hz60,b30hz70,b30hz80,b30hz90],axis=0,ignore_index=True)

# Healthy

healthy_df = pd.concat([h30hz0,h30hz10,h30hz20,h30hz30,h30hz40,h30hz50,h30hz60,h30hz70,h30hz80,h30hz90],axis=0,ignore_index=True)

# check
healthy_df.tail()

# check
broken_df.tail()

gear_data   = pd.concat([broken_df,healthy_df], axis =0)

gear_data.head()

gear_data.tail()

training_features = ['a1', 'a2', 'a3', 'a4', 'load']
label = ['failure']

x = gear_data[training_features]
x.shape #training features of consists of 2021119 rows and 5 columns

y = gear_data[label]
y.shape #label data consists of 2021119 rows and 1 col

x,y = shuffle(x,y)

1=x.head(60000)
y1=y.head(60000)

x1,y1 = shuffle(x1,y1)

x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.45, random_state=32)

x1_train.shape,x1_test.shape, y1_train.shape, y1_test.shape

classifier = svm.SVC(kernel='rbf', gamma='auto', C=2.0)

classifier.fit(x1_train, y1_train.values.ravel())

y_pred = classifier.predict(x1_test)

y1_pred = classifier.predict(x1_test)

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y1_test, y1_pred)

print("Confusion Matrix:")
print(conf_matrix)

print(accuracy_score(y1_test, y_pred))